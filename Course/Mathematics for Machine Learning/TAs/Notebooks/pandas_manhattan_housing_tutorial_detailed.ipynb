{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6561195",
   "metadata": {},
   "source": [
    "# ðŸ¼ Pandas Workshop â€” Manhattan Housing (Student Edition)\n",
    "\n",
    "Welcome! This notebook teaches **pandas** step-by-step using a real-world style dataset (we'll call it *Manhattan Housing*).  \n",
    "Itâ€™s designed for learning: every section includes **clear goals** and explains **what each code cell does** in plain language.\n",
    "\n",
    "> **Before you start:** Put your CSV file in the same folder as this notebook and name it `manhattan_housing.csv`.  \n",
    "> If your columns have different names (they probably will!), look for **TODO** comments and edit the names accordingly.\n",
    "\n",
    "### What we'll learn\n",
    "- Loading data into pandas and getting a first look\n",
    "- Selecting columns/rows; filtering with conditions\n",
    "- Handling missing values and fixing data types\n",
    "- Working with dates (month, year, resampling)\n",
    "- Feature engineering (e.g., price per square foot)\n",
    "- Grouping, aggregating, and pivot tables\n",
    "- Sorting, ranking, and window functions\n",
    "- Joining/merging with another table\n",
    "- String operations and categoricals\n",
    "- Plotting with matplotlib (simple and effective)\n",
    "- Performance tips\n",
    "- Practice exercises + worked solutions\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c04fd",
   "metadata": {},
   "source": [
    "## 0) Setup\n",
    "\n",
    "**Goal:** Import the libraries we'll use and check versions so everyone is on the same page.\n",
    "\n",
    "**What this does:**\n",
    "- Imports `pandas`, `numpy`, and `matplotlib.pyplot`\n",
    "- Prints the versions of Python, pandas, and numpy (helpful for debugging)\n",
    "- Enables inline plotting for charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2693b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Platform:', platform.platform())\n",
    "print('pandas:', pd.__version__)\n",
    "print('numpy:', np.__version__)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acdf54",
   "metadata": {},
   "source": [
    "## 1) Load the data\n",
    "\n",
    "**Goal:** Read a CSV file into a pandas **DataFrame**.\n",
    "\n",
    "**What this does:**\n",
    "- Reads `manhattan_housing.csv` into `df`\n",
    "- `low_memory=False` makes pandas read types more consistently\n",
    "- Shows the shape (rows Ã— columns)\n",
    "- Displays the first 5 rows to get a feel for the data\n",
    "\n",
    "> **TODO:** If your file isn't named `manhattan_housing.csv`, update the `CSV_PATH` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'manhattan_housing.csv'  # TODO: update if your file has a different name/path\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "print('Rows x Columns:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a738e99",
   "metadata": {},
   "source": [
    "## 2) First look: structure & schema\n",
    "\n",
    "**Goal:** Understand what columns exist, their data types, and peek at sample rows.\n",
    "\n",
    "**What this does:**\n",
    "- Lists column names and data types\n",
    "- Shows a random sample (to see varied rows)\n",
    "- Prints `info()` to view non-null counts and dtypes (great for spotting missing values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist(), df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837697c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(min(5, len(df)), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae93a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c43f1a",
   "metadata": {},
   "source": [
    "## 3) Selecting columns & rows\n",
    "\n",
    "**Goal:** Learn how to pick columns and filter rows.\n",
    "\n",
    "**What this does:**\n",
    "- Creates a smaller DataFrame `df_small` with columns of interest (edit the list below!)\n",
    "- Demonstrates `head()`, and basic filtering\n",
    "- Uses a condition to keep only rows with a positive sale price\n",
    "\n",
    "> **TODO:** Replace column names in `cols_of_interest` to match your dataset. Examples you might have:  \n",
    "> `sale_price`, `sale_date`, `neighborhood`, `gross_square_feet`, `land_square_feet`, `year_built`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072efbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns you care about (edit these to match your actual CSV)\n",
    "cols_of_interest = [\n",
    "    'sale_price',            # TODO\n",
    "    'sale_date',             # TODO\n",
    "    'neighborhood',          # TODO\n",
    "    'gross_square_feet',     # TODO\n",
    "    'land_square_feet',      # TODO\n",
    "    'year_built'             # TODO\n",
    "]\n",
    "\n",
    "existing_cols = [c for c in cols_of_interest if c in df.columns]\n",
    "df_small = df[existing_cols].copy() if existing_cols else df.copy()\n",
    "\n",
    "print('df_small columns:', df_small.columns.tolist())\n",
    "df_small.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter example: keep rows where sale price is positive (change the column name if needed)\n",
    "if 'sale_price' in df_small.columns:\n",
    "    df_pos = df_small[df_small['sale_price'] > 0]\n",
    "    print('Positive-price rows:', df_pos.shape[0])\n",
    "    df_pos.head()\n",
    "else:\n",
    "    print(\"TODO: Replace 'sale_price' with an existing numeric price column in your data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162da17",
   "metadata": {},
   "source": [
    "## 4) Cleaning & missing values\n",
    "\n",
    "**Goal:** See where values are missing and apply simple strategies to handle them.\n",
    "\n",
    "**What this does:**\n",
    "- Calculates the fraction of missing values per column\n",
    "- Creates `df_clean` by filling numeric columns with their median and text columns with their mode (most frequent value)\n",
    "- Re-checks missingness afterward\n",
    "\n",
    "> Thereâ€™s no one-size-fits-all approachâ€”this is just a straightforward starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.isna().mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_small.copy()\n",
    "for c in df_clean.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_clean[c]):\n",
    "        df_clean[c] = df_clean[c].fillna(df_clean[c].median())\n",
    "    else:\n",
    "        mode_vals = df_clean[c].mode()\n",
    "        df_clean[c] = df_clean[c].fillna(mode_vals.iloc[0] if len(mode_vals) else df_clean[c])\n",
    "df_clean.isna().mean().sort_values(ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c3860",
   "metadata": {},
   "source": [
    "## 5) Types & datetime handling\n",
    "\n",
    "**Goal:** Work with dates so we can group by month/year and do time-based analysis.\n",
    "\n",
    "**What this does:**\n",
    "- Converts a column to datetime (`sale_date`â€”change if needed)\n",
    "- Extracts `sale_year`, `sale_month`, and `sale_quarter` to use later for grouping\n",
    "\n",
    "> **TODO:** If your date column has a different name, change `'sale_date'` below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beff8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sale_date' in df_clean.columns:\n",
    "    df_clean['sale_date'] = pd.to_datetime(df_clean['sale_date'], errors='coerce', infer_datetime_format=True)\n",
    "    df_clean['sale_year'] = df_clean['sale_date'].dt.year\n",
    "    df_clean['sale_month'] = df_clean['sale_date'].dt.month\n",
    "    df_clean['sale_quarter'] = df_clean['sale_date'].dt.to_period('Q')\n",
    "    df_clean[['sale_date','sale_year','sale_month','sale_quarter']].head()\n",
    "else:\n",
    "    print(\"TODO: Replace 'sale_date' with your timestamp column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6c10a",
   "metadata": {},
   "source": [
    "## 6) Feature engineering\n",
    "\n",
    "**Goal:** Create useful derived columns for analysis.\n",
    "\n",
    "**What this does:**\n",
    "- Computes **price per square foot (ppsf)** from `sale_price / gross_square_feet`\n",
    "- Protects against division by zero and missing values\n",
    "\n",
    "> **TODO:** Update the column names if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if {'sale_price', 'gross_square_feet'} <= set(df_clean.columns):\n",
    "    df_clean['ppsf'] = df_clean['sale_price'] / df_clean['gross_square_feet'].replace(0, np.nan)\n",
    "    df_clean['ppsf'] = df_clean['ppsf'].replace([np.inf, -np.inf], np.nan)\n",
    "    df_clean[['sale_price','gross_square_feet','ppsf']].head()\n",
    "else:\n",
    "    print(\"TODO: Create a 'ppsf' feature using your price and area columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8022d",
   "metadata": {},
   "source": [
    "## 7) Groupby & aggregation\n",
    "\n",
    "**Goal:** Summarize data by categories (e.g., neighborhood and year).\n",
    "\n",
    "**What this does:**\n",
    "- Groups rows by `neighborhood` and `sale_year`\n",
    "- Calculates count, mean, median, min, max of `sale_price`\n",
    "- Sorts by mean price to see which groups are highest\n",
    "\n",
    "> **TODO:** Make sure the grouping and metric columns exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [c for c in ['neighborhood', 'sale_year'] if c in df_clean.columns]\n",
    "if group_cols and 'sale_price' in df_clean.columns:\n",
    "    g = (df_clean\n",
    "         .groupby(group_cols, dropna=False)['sale_price']\n",
    "         .agg(['count','mean','median','min','max'])\n",
    "         .sort_values('mean', ascending=False)\n",
    "        )\n",
    "    g.head(10)\n",
    "else:\n",
    "    print(\"TODO: Adjust 'group_cols' and the target metric for your dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d3976",
   "metadata": {},
   "source": [
    "## 8) Pivot tables\n",
    "\n",
    "**Goal:** Reshape data to compare values across two dimensions.\n",
    "\n",
    "**What this does:**\n",
    "- Creates a pivot table of **median PPSF** by `neighborhood` (rows) and `sale_year` (columns)\n",
    "\n",
    "> **Tip:** Pivot tables are great for heatmaps or quick comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f0568",
   "metadata": {},
   "outputs": [],
   "source": [
    "if {'neighborhood', 'sale_year', 'ppsf'} <= set(df_clean.columns):\n",
    "    pivot_ppsf = pd.pivot_table(\n",
    "        df_clean,\n",
    "        index='neighborhood',\n",
    "        columns='sale_year',\n",
    "        values='ppsf',\n",
    "        aggfunc='median'\n",
    "    )\n",
    "    pivot_ppsf.head()\n",
    "else:\n",
    "    print(\"TODO: Ensure 'neighborhood', 'sale_year', 'ppsf' exist for the pivot example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50e3cd6",
   "metadata": {},
   "source": [
    "## 9) Sorting, ranking, and window functions\n",
    "\n",
    "**Goal:** Order rows and find the \"top N\" within groups.\n",
    "\n",
    "**What this does:**\n",
    "- Sorts the DataFrame by price (descending)\n",
    "- Ranks sales within each `neighborhood` Ã— `sale_year` and shows the top 3\n",
    "\n",
    "> **Why this matters:** Window functions help you compare rows within their peer groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df82b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sale_price' in df_clean.columns:\n",
    "    df_sorted = df_clean.sort_values('sale_price', ascending=False)\n",
    "    df_sorted.head(10)\n",
    "else:\n",
    "    print(\"TODO: Replace 'sale_price' with your target sort column.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if {'neighborhood', 'sale_year', 'sale_price'} <= set(df_clean.columns):\n",
    "    df_clean = df_clean.sort_values(['neighborhood','sale_year','sale_price'], ascending=[True, True, False])\n",
    "    df_clean['rank_in_nbhd_year'] = df_clean.groupby(['neighborhood','sale_year'])['sale_price'].rank(method='first', ascending=False)\n",
    "    df_clean[df_clean['rank_in_nbhd_year'] <= 3].head(10)\n",
    "else:\n",
    "    print(\"TODO: Ensure grouping and value columns exist for ranking demo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd118f",
   "metadata": {},
   "source": [
    "## 10) Joins & merges (optional)\n",
    "\n",
    "**Goal:** Combine two tables using a shared key.\n",
    "\n",
    "**What this does:**\n",
    "- Creates a small \"supplemental\" table with a `zip_code` and a fake `park_access_score`\n",
    "- Merges it into the main data on `zip_code` using a left join\n",
    "\n",
    "> In real projects, you'd load a second CSV and merge on a real key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a53eb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "supp = pd.DataFrame({\n",
    "    'zip_code': [10001, 10002, 10003],\n",
    "    'park_access_score': [72, 64, 81]\n",
    "})\n",
    "if 'zip_code' in df_clean.columns:\n",
    "    df_merged = df_clean.merge(supp, on='zip_code', how='left')\n",
    "    df_merged[['zip_code','park_access_score']].head()\n",
    "else:\n",
    "    print(\"Optional: add a ZIP code column to demo merges.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa0937",
   "metadata": {},
   "source": [
    "## 11) String operations & categoricals\n",
    "\n",
    "**Goal:** Clean text columns and improve performance with categoricals.\n",
    "\n",
    "**What this does:**\n",
    "- Standardizes `neighborhood` names (strip whitespace, Title Case)\n",
    "- Converts some object (string) columns to **category** type to save memory\n",
    "\n",
    "> **Tip:** Categoricals are great for columns with repeated labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e465f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String cleaning example\n",
    "if 'neighborhood' in df_clean.columns:\n",
    "    df_clean['neighborhood'] = df_clean['neighborhood'].astype(str).str.strip().str.title()\n",
    "    df_clean['neighborhood'].head()\n",
    "else:\n",
    "    print(\"Optional: add a 'neighborhood' string column to demo string ops.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert object columns to category (where sensible)\n",
    "for c in ['neighborhood','borough','building_class_category']:\n",
    "    if c in df_clean.columns and df_clean[c].dtype == 'object':\n",
    "        df_clean[c] = df_clean[c].astype('category')\n",
    "df_clean.info(memory_usage='deep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e24bf",
   "metadata": {},
   "source": [
    "## 12) Plotting with matplotlib\n",
    "\n",
    "**Goal:** Visualize key patterns with simple charts.\n",
    "\n",
    "**What this does:**\n",
    "- Plots a histogram of `sale_price`\n",
    "- Plots a monthly median `sale_price` time series (if a date column was parsed)\n",
    "- Plots a bar chart of the top 10 neighborhoods by median PPSF\n",
    "\n",
    "> We use plain matplotlib here. (No seaborn, and we don't set any custom colors.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.1 Histogram of sale prices\n",
    "if 'sale_price' in df_clean.columns:\n",
    "    plt.figure()\n",
    "    df_clean['sale_price'].dropna().plot(kind='hist', bins=50, title='Distribution of Sale Price')\n",
    "else:\n",
    "    print(\"TODO: Provide a numeric price column for the histogram.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330689ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.2 Time series: monthly median price\n",
    "if {'sale_date','sale_price'} <= set(df_clean.columns):\n",
    "    monthly = (df_clean\n",
    "               .set_index('sale_date')\n",
    "               .resample('MS')['sale_price']\n",
    "               .median())\n",
    "    plt.figure()\n",
    "    monthly.plot(title='Monthly Median Sale Price')\n",
    "else:\n",
    "    print(\"TODO: Ensure 'sale_date' is parsed to datetime for resampling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b47be",
   "metadata": {},
   "source": [
    "**Interpretation tips:**  \n",
    "- Histograms show spread and outliers (e.g., unusually high sales).  \n",
    "- Time series help identify trends or seasonality.  \n",
    "- Bar charts make categorical comparisons easy (e.g., neighborhoods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12.3 Bar chart: top neighborhoods by median PPSF\n",
    "if {'neighborhood','ppsf'} <= set(df_clean.columns):\n",
    "    nbhd_ppsf = (df_clean\n",
    "                 .groupby('neighborhood')['ppsf']\n",
    "                 .median()\n",
    "                 .sort_values(ascending=False)\n",
    "                 .head(10))\n",
    "    plt.figure()\n",
    "    nbhd_ppsf.plot(kind='bar', title='Top 10 Neighborhoods by Median PPSF')\n",
    "else:\n",
    "    print(\"TODO: Create 'ppsf' and 'neighborhood' columns for this plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47936b22",
   "metadata": {},
   "source": [
    "## 13) Performance & memory tips\n",
    "\n",
    "**Goal:** Learn how to keep analyses fast and memory-efficient.\n",
    "\n",
    "**What this does:**\n",
    "- Shows memory usage by column (so you can target the biggest ones)\n",
    "- Demonstrates vectorization vs. `apply` (vectorized operations are usually faster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c4728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage by column (largest first)\n",
    "mem = df_clean.memory_usage(deep=True).sort_values(ascending=False)\n",
    "mem.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ebdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization vs. apply micro-benchmark (small demo)\n",
    "import time\n",
    "if 'sale_price' in df_clean.columns:\n",
    "    s = df_clean['sale_price'].fillna(0).head(100000) if len(df_clean) > 0 else pd.Series([])\n",
    "    start = time.time()\n",
    "    v = s * 1.05  # vectorized 5% increase\n",
    "    t_vec = time.time() - start\n",
    "\n",
    "    start = time.time()\n",
    "    a = s.apply(lambda x: x * 1.05)\n",
    "    t_apply = time.time() - start\n",
    "\n",
    "    print(f\"Vectorized: {t_vec:.6f}s | apply: {t_apply:.6f}s (smaller is faster)\")\n",
    "else:\n",
    "    print(\"Optional: provide a numeric column to demo vectorization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74813f",
   "metadata": {},
   "source": [
    "## 14) Exercises (practice)\n",
    "\n",
    "Try these on your own first. Use the code above as a reference and adjust column names to your dataset.\n",
    "\n",
    "1. **Data hygiene**:  \n",
    "   - Drop rows with nonâ€‘positive `sale_price`.  \n",
    "   - Remove outliers: keep only the 1stâ€“99th percentiles of `sale_price`.\n",
    "2. **Datetime**:  \n",
    "   - Parse `sale_date` and create `sale_year`, `sale_month`.\n",
    "3. **Aggregation**:  \n",
    "   - Compute median `ppsf` by `neighborhood` and list the top 5.\n",
    "4. **Window functions**:  \n",
    "   - Within each `neighborhood`, rank sales by `sale_price` and select the top 3 per neighborhood.\n",
    "5. **Visualization**:  \n",
    "   - Plot a time series of monthly median `sale_price`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7608976",
   "metadata": {},
   "source": [
    "## 15) Exercise solutions (reveal only after trying)\n",
    "\n",
    "Below are straightforward solutions for the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E1 solution â€” Data hygiene\n",
    "df_sol = df.copy()\n",
    "if 'sale_price' in df_sol.columns:\n",
    "    df_sol = df_sol[df_sol['sale_price'] > 0]\n",
    "    lo, hi = df_sol['sale_price'].quantile([0.01, 0.99])\n",
    "    df_sol = df_sol[df_sol['sale_price'].between(lo, hi)]\n",
    "    df_sol.head()\n",
    "else:\n",
    "    print(\"Update 'sale_price' to match your dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33107ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2 solution â€” Datetime parsing\n",
    "if 'sale_date' in df_sol.columns:\n",
    "    df_sol['sale_date'] = pd.to_datetime(df_sol['sale_date'], errors='coerce', infer_datetime_format=True)\n",
    "    df_sol['sale_year'] = df_sol['sale_date'].dt.year\n",
    "    df_sol['sale_month'] = df_sol['sale_date'].dt.month\n",
    "    df_sol[['sale_date','sale_year','sale_month']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ef88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E3 solution â€” Aggregation\n",
    "if {'sale_price','gross_square_feet'} <= set(df_sol.columns):\n",
    "    df_sol['ppsf'] = df_sol['sale_price'] / df_sol['gross_square_feet'].replace(0, np.nan)\n",
    "    top5 = (df_sol.groupby('neighborhood')['ppsf']\n",
    "            .median()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(5))\n",
    "    top5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8076f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E4 solution â€” Window functions\n",
    "if {'neighborhood','sale_price'} <= set(df_sol.columns):\n",
    "    df_sol = df_sol.sort_values(['neighborhood','sale_price'], ascending=[True, False])\n",
    "    df_sol['rank_in_nbhd'] = df_sol.groupby('neighborhood')['sale_price'].rank(method='first', ascending=False)\n",
    "    df_sol[df_sol['rank_in_nbhd'] <= 3].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99dc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E5 solution â€” Visualization\n",
    "if {'sale_date','sale_price'} <= set(df_sol.columns):\n",
    "    monthly = (df_sol\n",
    "               .set_index('sale_date')\n",
    "               .resample('MS')['sale_price']\n",
    "               .median())\n",
    "    plt.figure()\n",
    "    monthly.plot(title='Monthly Median Sale Price (Solution)')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
